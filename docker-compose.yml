services:
  # Python Orchestration Service
  raggedy-orchestrator:
    build: .
    container_name: raggedy-orchestrator
    volumes:
      - ./data:/app/data
      - ./raw:/app/raw
      - ./processed:/app/processed
      - ./models:/app/models
      - ./logs:/app/logs
    ports:
      - "8501:8501"  # Streamlit UI
    depends_on:
      elasticsearch:
        condition: service_healthy
      qdrant:
        condition: service_started
      neo4j:
        condition: service_started
      llama-cpp:
        condition: service_started
    environment:
      - ELASTICSEARCH_HOST=elasticsearch:9200
      - QDRANT_HOST=qdrant:6333
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
      - LLAMA_CPP_HOST=llama-cpp:8000
    networks:
      - raggedy-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # Elasticsearch for lexical search
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: raggedy-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - raggedy-network

  # Qdrant for vector database
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: raggedy-qdrant
    volumes:
      - qdrantdata:/qdrant/storage
    ports:
      - "6333:6333"
    networks:
      - raggedy-network

  # Neo4j for knowledge graph
  neo4j:
    image: neo4j:5.18
    container_name: raggedy-neo4j
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["graph-data-science"]
    volumes:
      - neo4jdata:/data
    ports:
      - "7474:7474"
      - "7687:7687"
    networks:
      - raggedy-network

  # llama.cpp for LLM inference
  llama-cpp:
    image: ghcr.io/abetlen/llama-cpp-python:latest
    container_name: raggedy-llama-cpp
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
    environment:
      - MODEL=/models/Hamanasu-QWQ-V2-RP.i1-Q4_K_M.gguf
    networks:
      - raggedy-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  esdata:
  qdrantdata:
  neo4jdata:

networks:
  raggedy-network:
    driver: bridge